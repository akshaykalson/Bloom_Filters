# -*- coding: utf-8 -*-
"""Computing at scale_problem set 5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hQ-mxjvqx6mDWGUmD5pPZ_uPuz3boVcV
"""

!pip install pybloom-live
import random
from pybloom_live import BloomFilter

# Function to generate a large set of unique book IDs
def generate_book_ids(count):
    book_id_set = set()
    while len(book_id_set) < count:
        book_id_set.add(''.join(random.choices('abcdefghijklmnopqrstuvwxyz', k=10)))
    return book_id_set

# Generate 1 billion unique book IDs
total_book_ids = 1000000000
all_book_ids = generate_book_ids(total_book_ids)

# Divide book IDs among branches (for demonstration purposes)
branch_size = total_book_ids // 3
central_campus_books = list(all_book_ids)[:branch_size]
sachsendorf_books = list(all_book_ids)[branch_size:branch_size * 2]
senftenberg_books = list(all_book_ids)[branch_size * 2:]

# Create Bloom filters for each branch
central_campus_filter = BloomFilter(capacity=branch_size, error_rate=0.001)
for book_id in central_campus_books:
    central_campus_filter.add(book_id)

sachsendorf_filter = BloomFilter(capacity=branch_size, error_rate=0.001)
for book_id in sachsendorf_books:
    sachsendorf_filter.add(book_id)

senftenberg_filter = BloomFilter(capacity=branch_size, error_rate=0.001)
for book_id in senftenberg_books:
    senftenberg_filter.add(book_id)

# Combine Bloom filters from all branches into a centralized filter
centralized_filter = central_campus_filter.union(sachsendorf_filter)
centralized_filter = centralized_filter.union(senftenberg_filter)

# Querying the centralized Bloom filter
query_book_id = random.choice(list(all_book_ids))  # Randomly select a book ID for querying
if query_book_id in centralized_filter:
    print(f"Book ID {query_book_id} might exist in the library.")
else:
    print(f"Book ID {query_book_id} does not exist in the library.")